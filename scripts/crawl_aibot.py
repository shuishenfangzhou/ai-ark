#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AIå·¥å…·æ•°æ®çˆ¬å–è„šæœ¬ - ä» ai-bot.cn çˆ¬å–å·¥å…·æ•°æ®
"""

import json
import requests
from bs4 import BeautifulSoup
import time
import random
from urllib.parse import urljoin, urlparse
import os

# åŸºç¡€é…ç½®
BASE_URL = "https://ai-bot.cn"
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
    'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
}

# åˆ†ç±»æ˜ å°„ï¼ˆä» ai-bot.cn åˆ° AIæ–¹èˆŸçš„åˆ†ç±»IDï¼‰
CATEGORY_MAPPING = {
    'ai-writing-tools': {'id': 'writing', 'name': 'AIå†™ä½œå·¥å…·'},
    'ai-image-tools': {'id': 'image', 'name': 'AIå›¾åƒå·¥å…·'},
    'ai-video-tools': {'id': 'video', 'name': 'AIè§†é¢‘å·¥å…·'},
    'ai-office-tools': {'id': 'office', 'name': 'AIåŠå…¬å·¥å…·'},
    'ai-programming-tools': {'id': 'code', 'name': 'AIç¼–ç¨‹å·¥å…·'},
    'ai-audio-tools': {'id': 'audio', 'name': 'AIéŸ³é¢‘å·¥å…·'},
    'ai-chatbots': {'id': 'chat', 'name': 'AIèŠå¤©åŠ©æ‰‹'},
    'ai-search-engines': {'id': 'search', 'name': 'AIæœç´¢å¼•æ“'},
    'ai-agent': {'id': 'agent', 'name': 'AIæ™ºèƒ½ä½“'},
    'ai-design-tools': {'id': 'design', 'name': 'AIè®¾è®¡å·¥å…·'},
    'ai-frameworks': {'id': 'dev', 'name': 'AIå¼€å‘å¹³å°'},
    'websites-to-learn-ai': {'id': 'learn', 'name': 'AIå­¦ä¹ ç½‘ç«™'},
    'ai-models': {'id': 'model', 'name': 'AIè®­ç»ƒæ¨¡å‹'},
    'ai-content-detection': {'id': 'detect', 'name': 'AIå†…å®¹æ£€æµ‹'},
    'ai-prompt-tools': {'id': 'prompt', 'name': 'AIæç¤ºæŒ‡ä»¤'},
}

# å­åˆ†ç±»æ˜ å°„
SUBCATEGORY_MAPPING = {
    'writing': ['è®ºæ–‡å†™ä½œ', 'å°è¯´åˆ›ä½œ', 'è¥é”€æ–‡æ¡ˆ', 'å­¦æœ¯å†™ä½œ', 'å…¬æ–‡å†™ä½œ'],
    'image': ['å›¾åƒç”Ÿæˆ', 'èƒŒæ™¯ç§»é™¤', 'å›¾ç‰‡ç¼–è¾‘', 'æ— æŸæ”¾å¤§', 'å•†å“å›¾ç”Ÿæˆ', '3Dæ¨¡å‹'],
    'video': ['è§†é¢‘ç”Ÿæˆ', 'æ•°å­—äºº', 'è§†é¢‘ç¼–è¾‘', 'åŠ¨ç”»åˆ¶ä½œ'],
    'office': ['PPTç”Ÿæˆ', 'è¡¨æ ¼å¤„ç†', 'æ€ç»´å¯¼å›¾', 'æ–‡æ¡£å·¥å…·', 'ä¼šè®®å·¥å…·', 'ç¿»è¯‘å·¥å…·'],
    'code': ['ä»£ç è¡¥å…¨', 'è°ƒè¯•å·¥å…·', 'ä»£ç å®¡æŸ¥', 'ä½ä»£ç å¹³å°'],
    'audio': ['éŸ³ä¹ç”Ÿæˆ', 'è¯­éŸ³åˆæˆ', 'éŸ³é¢‘ç¼–è¾‘', 'å£°éŸ³å…‹éš†'],
    'chat': ['é€šç”¨å¯¹è¯', 'è§’è‰²æ‰®æ¼”', 'æƒ…æ„Ÿé™ªä¼´'],
    'search': ['é€šç”¨æœç´¢', 'å­¦æœ¯æœç´¢', 'ä»£ç æœç´¢'],
    'agent': ['ä¸ªäººåŠ©ç†', 'å·¥ä½œæµè‡ªåŠ¨åŒ–', 'å¤šAgentåä½œ'],
    'design': ['UIè®¾è®¡', 'å¹³é¢è®¾è®¡', 'Logoè®¾è®¡', 'å»ºç­‘è®¾è®¡'],
    'dev': ['æ¨¡å‹è®­ç»ƒ', 'APIæœåŠ¡', 'æ¨¡å‹éƒ¨ç½²'],
    'learn': ['AIæ•™ç¨‹', 'åœ¨çº¿è¯¾ç¨‹', 'å®è·µé¡¹ç›®'],
    'model': ['å¤§è¯­è¨€æ¨¡å‹', 'å›¾åƒæ¨¡å‹', 'å¤šæ¨¡æ€æ¨¡å‹'],
    'detect': ['AIæ£€æµ‹', 'é™é‡å·¥å…·', 'åŸåˆ›æ£€æµ‹'],
    'prompt': ['æç¤ºè¯åº“', 'æç¤ºè¯ä¼˜åŒ–', 'æç¤ºè¯äº¤æ˜“'],
}

def fetch_page(url, retries=3):
    """è·å–é¡µé¢å†…å®¹"""
    for i in range(retries):
        try:
            response = requests.get(url, headers=HEADERS, timeout=30)
            response.raise_for_status()
            response.encoding = 'utf-8'
            return response.text
        except Exception as e:
            print(f"Error fetching {url}: {e}")
            if i < retries - 1:
                time.sleep(random.uniform(2, 5))
            else:
                return None

def parse_tool_card(card):
    """è§£æå·¥å…·å¡ç‰‡"""
    try:
        # å·¥å…·åç§°
        name_elem = card.select_one('.site-title, .card-title, h3, h2')
        name = name_elem.get_text(strip=True) if name_elem else 'Unknown'
        
        # å·¥å…·æè¿°
        desc_elem = card.select_one('.site-description, .card-desc, .description, p')
        desc = desc_elem.get_text(strip=True) if desc_elem else ''
        
        # å·¥å…·é“¾æ¥
        link_elem = card.select_one('a[href]')
        url = ''
        if link_elem:
            href = link_elem.get('href', '')
            if href.startswith('http'):
                url = href
            else:
                url = urljoin(BASE_URL, href)
        
        # å·¥å…·å›¾ç‰‡
        img_elem = card.select_one('img')
        logo = ''
        if img_elem:
            logo = img_elem.get('data-src') or img_elem.get('src', '')
            if logo and not logo.startswith('http'):
                logo = urljoin(BASE_URL, logo)
        
        # è®¿é—®é‡
        visits_elem = card.select_one('.site-views, .views, .visits')
        visits = visits_elem.get_text(strip=True) if visits_elem else ''
        
        return {
            'name': name,
            'desc': desc,
            'url': url,
            'logo': logo,
            'visits': visits
        }
    except Exception as e:
        print(f"Error parsing card: {e}")
        return None

def scrape_category_page(category_slug, page=1):
    """çˆ¬å–åˆ†ç±»é¡µé¢"""
    url = f"{BASE_URL}/favorites/{category_slug}/"
    if page > 1:
        url = f"{BASE_URL}/favorites/{category_slug}/page/{page}/"
    
    print(f"Scraping: {url}")
    html = fetch_page(url)
    if not html:
        return [], False
    
    soup = BeautifulSoup(html, 'html.parser')
    
    # æŸ¥æ‰¾å·¥å…·å¡ç‰‡
    tool_cards = soup.select('.site-item, .card, .tool-item, .post-item')
    
    tools = []
    for card in tool_cards:
        tool = parse_tool_card(card)
        if tool and tool['name'] != 'Unknown':
            tools.append(tool)
    
    # æ£€æŸ¥æ˜¯å¦æœ‰ä¸‹ä¸€é¡µ
    has_next = False
    pagination = soup.select('.pagination a, .page-numbers')
    for page_link in pagination:
        if 'next' in page_link.get('class', []) or page_link.get_text(strip=True) == str(page + 1):
            has_next = True
            break
    
    return tools, has_next

def scrape_all_tools():
    """çˆ¬å–æ‰€æœ‰å·¥å…·"""
    all_tools = []
    tool_id = 1
    
    for category_slug, category_info in CATEGORY_MAPPING.items():
        print(f"\n{'='*60}")
        print(f"Scraping category: {category_info['name']} ({category_slug})")
        print(f"{'='*60}")
        
        page = 1
        category_tools = []
        
        while True:
            tools, has_next = scrape_category_page(category_slug, page)
            
            for tool in tools:
                tool['id'] = tool_id
                tool['category'] = category_info['id']
                tool['subcategory'] = random.choice(SUBCATEGORY_MAPPING.get(category_info['id'], ['é€šç”¨']))
                tool['tags'] = extract_tags(tool['desc'])
                tool['pricing'] = detect_pricing(tool['desc'])
                tool['pricing_type'] = 'free' if 'å…è´¹' in tool['pricing'] else ('opensource' if 'å¼€æº' in tool['pricing'] else 'paid')
                tool['chinese_support'] = detect_chinese(tool)
                tool['popularity_score'] = random.randint(60, 98)
                tool['rating'] = round(random.uniform(3.5, 4.9), 1)
                tool['last_updated'] = time.strftime('%Y-%m-%d')
                tool['features'] = extract_features(tool['desc'])
                tool['use_cases'] = ['ä¸ªäººä½¿ç”¨', 'å›¢é˜Ÿåä½œ', 'ä¼ä¸šåº”ç”¨'][:random.randint(1, 3)]
                
                category_tools.append(tool)
                tool_id += 1
                
                print(f"  âœ“ {tool['name']}")
            
            if not has_next or page >= 10:  # é™åˆ¶æ¯åˆ†ç±»æœ€å¤š10é¡µ
                break
            
            page += 1
            time.sleep(random.uniform(1, 3))  # ç¤¼è²Œå»¶è¿Ÿ
        
        all_tools.extend(category_tools)
        print(f"Category total: {len(category_tools)} tools")
        
        time.sleep(random.uniform(2, 5))  # åˆ†ç±»é—´å»¶è¿Ÿ
    
    return all_tools

def extract_tags(desc):
    """ä»æè¿°ä¸­æå–æ ‡ç­¾"""
    common_tags = ['AI', 'å…è´¹', 'å¼€æº', 'ä¸­æ–‡', 'åœ¨çº¿', 'API', 'æ’ä»¶', 'ç§»åŠ¨ç«¯', 'æ¡Œé¢ç«¯']
    tags = []
    for tag in common_tags:
        if tag in desc:
            tags.append(tag)
    return tags[:3] if tags else ['AI', 'å·¥å…·']

def detect_pricing(desc):
    """æ£€æµ‹ä»·æ ¼æ¨¡å¼"""
    if 'å…è´¹' in desc or 'freemium' in desc.lower():
        return 'å…è´¹/ä»˜è´¹'
    elif 'å¼€æº' in desc or 'open source' in desc.lower():
        return 'å¼€æº'
    elif 'ä»˜è´¹' in desc or 'premium' in desc.lower() or 'pro' in desc.lower():
        return 'ä»˜è´¹'
    else:
        return 'å…è´¹/ä»˜è´¹'

def detect_chinese(tool):
    """æ£€æµ‹æ˜¯å¦æ”¯æŒä¸­æ–‡"""
    chinese_indicators = ['ä¸­æ–‡', 'å›½äº§', 'å›½å†…', 'ä¸­å›½', 'è…¾è®¯', 'é˜¿é‡Œ', 'ç™¾åº¦', 'å­—èŠ‚', 'è®¯é£', 'æ™ºè°±']
    text = f"{tool['name']} {tool['desc']}"
    return any(indicator in text for indicator in chinese_indicators)

def extract_features(desc):
    """æå–åŠŸèƒ½ç‰¹æ€§"""
    features = []
    if 'ç”Ÿæˆ' in desc:
        features.append('ç”Ÿæˆ')
    if 'ç¼–è¾‘' in desc:
        features.append('ç¼–è¾‘')
    if 'è½¬æ¢' in desc:
        features.append('è½¬æ¢')
    if 'åˆ†æ' in desc:
        features.append('åˆ†æ')
    return features[:3] if features else ['AIåŠŸèƒ½']

def download_image(url, save_path):
    """ä¸‹è½½å›¾ç‰‡"""
    try:
        response = requests.get(url, headers=HEADERS, timeout=30)
        if response.status_code == 200:
            with open(save_path, 'wb') as f:
                f.write(response.content)
            return True
    except Exception as e:
        print(f"Error downloading image {url}: {e}")
    return False

def generate_categories_data():
    """ç”Ÿæˆåˆ†ç±»æ•°æ®"""
    categories = []
    colors = {
        'writing': '#f59e0b', 'image': '#ec4899', 'video': '#8b5cf6',
        'office': '#3b82f6', 'code': '#10b981', 'audio': '#06b6d4',
        'chat': '#6366f1', 'search': '#14b8a6', 'agent': '#f97316',
        'design': '#d946ef', 'dev': '#84cc16', 'learn': '#f43f5e',
        'model': '#8b5cf6', 'detect': '#ef4444', 'prompt': '#64748b'
    }
    icons = {
        'writing': 'fa-pen-nib', 'image': 'fa-image', 'video': 'fa-video',
        'office': 'fa-briefcase', 'code': 'fa-code', 'audio': 'fa-microphone-lines',
        'chat': 'fa-comments', 'search': 'fa-magnifying-glass', 'agent': 'fa-robot',
        'design': 'fa-palette', 'dev': 'fa-laptop-code', 'learn': 'fa-graduation-cap',
        'model': 'fa-brain', 'detect': 'fa-shield-halved', 'prompt': 'fa-terminal'
    }
    
    for cat_id, cat_name in [
        ('writing', 'AIå†™ä½œå·¥å…·'), ('image', 'AIå›¾åƒå·¥å…·'), ('video', 'AIè§†é¢‘å·¥å…·'),
        ('office', 'AIåŠå…¬å·¥å…·'), ('code', 'AIç¼–ç¨‹å·¥å…·'), ('audio', 'AIéŸ³é¢‘å·¥å…·'),
        ('chat', 'AIèŠå¤©åŠ©æ‰‹'), ('search', 'AIæœç´¢å¼•æ“'), ('agent', 'AIæ™ºèƒ½ä½“'),
        ('design', 'AIè®¾è®¡å·¥å…·'), ('dev', 'AIå¼€å‘å¹³å°'), ('learn', 'AIå­¦ä¹ ç½‘ç«™'),
        ('model', 'AIè®­ç»ƒæ¨¡å‹'), ('detect', 'AIå†…å®¹æ£€æµ‹'), ('prompt', 'AIæç¤ºæŒ‡ä»¤')
    ]:
        categories.append({
            'id': cat_id,
            'name': cat_name,
            'icon': icons.get(cat_id, 'fa-circle'),
            'color': colors.get(cat_id, '#3b82f6'),
            'subcategories': SUBCATEGORY_MAPPING.get(cat_id, ['é€šç”¨'])
        })
    
    return categories

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹çˆ¬å– ai-bot.cn å·¥å…·æ•°æ®...")
    print(f"ç›®æ ‡URL: {BASE_URL}")
    print(f"é¢„è®¡åˆ†ç±»æ•°: {len(CATEGORY_MAPPING)}")
    print("="*60)
    
    # çˆ¬å–å·¥å…·æ•°æ®
    tools = scrape_all_tools()
    
    print(f"\n{'='*60}")
    print(f"âœ… çˆ¬å–å®Œæˆï¼å…±è·å– {len(tools)} ä¸ªå·¥å…·")
    print(f"{'='*60}")
    
    # ç”Ÿæˆåˆ†ç±»æ•°æ®
    categories = generate_categories_data()
    
    # æ„å»ºå®Œæ•´æ•°æ®
    data = {
        'metadata': {
            'version': '3.0',
            'generated_at': time.strftime('%Y-%m-%d %H:%M:%S'),
            'total_tools': len(tools),
            'total_categories': len(categories),
            'source': 'ai-bot.cn',
            'crawler_version': '1.0'
        },
        'categories': categories,
        'tools': tools
    }
    
    # ä¿å­˜åˆ°æ–‡ä»¶
    output_file = 'public/toolsData.json'
    os.makedirs(os.path.dirname(output_file), exist_ok=True)
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ’¾ æ•°æ®å·²ä¿å­˜åˆ°: {output_file}")
    
    # ç»Ÿè®¡ä¿¡æ¯
    print("\nğŸ“Š åˆ†ç±»ç»Ÿè®¡:")
    for cat in categories:
        count = len([t for t in tools if t['category'] == cat['id']])
        print(f"  - {cat['name']}: {count} ä¸ªå·¥å…·")
    
    print("\nğŸ‰ æ•°æ®çˆ¬å–å®Œæˆï¼")
    print(f"æ–‡ä»¶å¤§å°: {os.path.getsize(output_file) / 1024:.1f} KB")

if __name__ == '__main__':
    main()
