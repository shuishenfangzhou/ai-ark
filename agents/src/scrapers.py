"""
Data Scrapers
ä»ä¸åŒæ¥æºæŠ“å– AI å·¥å…·æ•°æ®
"""
import os
import json
import re
import httpx
from bs4 import BeautifulSoup
from typing import List, Dict, Any, Optional
from datetime import datetime, timedelta
from abc import ABC, abstractmethod


class BaseScraper(ABC):
    """æŠ“å–å™¨åŸºç±»"""
    
    def __init__(self, name: str):
        self.name = name
        self.client = httpx.AsyncClient(timeout=30.0)
    
    @abstractmethod
    async def fetch(self, **kwargs) -> List[Dict[str, Any]]:
        """è·å–æ•°æ®ï¼ˆå­ç±»å¿…é¡»å®ç°ï¼‰"""
        pass
    
    async def close(self):
        """å…³é—­ HTTP å®¢æˆ·ç«¯"""
        await self.client.aclose()
    
    def _clean_text(self, text: str) -> str:
        """æ¸…ç†æ–‡æœ¬"""
        if not text:
            return ""
        return re.sub(r'\s+', ' ', text).strip()
    
    def _extract_stars(self, text: str) -> int:
        """æå– star æ•°é‡"""
        if not text:
            return 0
        match = re.search(r'([\d,]+)\s*stars?', text, re.I)
        if match:
            return int(match.group(1).replace(',', ''))
        return 0


class GitHubTrendingScraper(BaseScraper):
    """GitHub Trending æŠ“å–å™¨"""
    
    def __init__(self):
        super().__init__("GitHub Trending")
        self.base_url = "https://github.com"
    
    async def fetch(self, language: str = "python", 
                    time_range: str = "daily") -> List[Dict[str, Any]]:
        """æŠ“å– GitHub Trending"""
        url = f"{self.base_url}/trending/{language}"
        if time_range != "daily":
            url += f"?since={time_range}"
        
        try:
            response = await self.client.get(url, headers={
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
            })
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'lxml')
            repos = []
            
            # è§£æä»“åº“åˆ—è¡¨
            for article in soup.select('article.box-border'):
                try:
                    repo_data = self._parse_repo(article)
                    if repo_data:
                        repos.append(repo_data)
                except Exception as e:
                    print(f"è§£æä»“åº“å¤±è´¥: {e}")
                    continue
            
            return repos
        except Exception as e:
            print(f"GitHub Trending æŠ“å–å¤±è´¥: {e}")
            return []
    
    def _parse_repo(self, article) -> Dict[str, Any]:
        """è§£æå•ä¸ªä»“åº“ä¿¡æ¯"""
        # è·å–ä»“åº“åç§°
        title_elem = article.select_one('h2 a')
        if not title_elem:
            return None
        
        full_name = self._clean_text(title_elem.get('href', '')).strip('/')
        name = full_name.split('/')[-1]
        
        # è·å–æè¿°
        desc_elem = article.select_one('p')
        description = self._clean_text(desc_elem.get_text()) if desc_elem else ""
        
        # è·å–è¯­è¨€å’Œæ˜Ÿæ ‡
        lang_elem = article.select_one('[itemprop="programmingLanguage"]')
        language = self._clean_text(lang_elem.get_text()) if lang_elem else ""
        
        stars_text = article.select_one('a[href$="stargazers"]')
        stars = self._extract_stars(stars_text.get_text()) if stars_text else 0
        
        # æ„å»ºç»“æœ
        return {
            "id": f"github-{full_name.replace('/', '-')}",
            "name": name,
            "name_en": name,
            "description": description,
            "url": f"{self.base_url}/{full_name}",
            "category": "AIç¼–ç¨‹" if "ai" in language.lower() else "AIå·¥å…·",
            "tags": ["GitHub", "å¼€æº", language] if language else ["GitHub", "å¼€æº"],
            "pricing": "opensource",
            "rating": min(5.0, stars / 1000 * 3) if stars > 0 else 3.0,
            "popularity": stars,
            "source": "github",
        }


class ProductHuntScraper(BaseScraper):
    """Product Hunt æŠ“å–å™¨"""
    
    def __init__(self):
        super().__init__("Product Hunt")
        self.base_url = "https://www.producthunt.com"
    
    async def fetch(self, category: str = "ai") -> List[Dict[str, Any]]:
        """æŠ“å– Product Hunt"""
        # Product Hunt æœ‰ APIï¼Œä½†è¿™é‡Œä½¿ç”¨ç½‘é¡µæŠ“å–ä½œä¸ºå¤‡é€‰
        url = f"{self.base_url}/categories/{category}"
        
        try:
            response = await self.client.get(url, headers={
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
            })
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'lxml')
            products = []
            
            for post in soup.select('[data-test="category-post-item"]'):
                try:
                    product_data = self._parse_product(post)
                    if product_data:
                        products.append(product_data)
                except Exception as e:
                    print(f"è§£æäº§å“å¤±è´¥: {e}")
                    continue
            
            return products
        except Exception as e:
            print(f"Product Hunt æŠ“å–å¤±è´¥: {e}")
            return []
    
    def _parse_product(self, post) -> Dict[str, Any]:
        """è§£æå•ä¸ªäº§å“ä¿¡æ¯"""
        # äº§å“åç§°
        name_elem = post.select_one('h3')
        name = self._clean_text(name_elem.get_text()) if name_elem else ""
        
        # æ ‡è¯­
        tagline_elem = post.select_one('[data-test="post-tagline"]')
        tagline = self._clean_text(tagline_elem.get_text()) if tagline_elem else ""
        
        # é“¾æ¥
        link_elem = post.select_one('a[href^="/posts/"]')
        slug = link_elem.get('href', '').split('/')[-1] if link_elem else ""
        
        # Vote æ•°é‡
        vote_elem = post.select_one('[data-test="vote-count"]')
        votes = 0
        if vote_elem:
            vote_text = self._clean_text(vote_elem.get_text())
            votes = int(vote_text.replace(',', '')) if vote_text.isdigit() else 0
        
        return {
            "id": f"ph-{slug}",
            "name": name,
            "description": tagline,
            "url": f"{self.base_url}/posts/{slug}",
            "category": "AIäº§å“",
            "tags": ["Product Hunt", "æ–°å“"],
            "pricing": "freemium",
            "rating": min(5.0, votes / 100) if votes > 0 else 3.0,
            "popularity": votes,
            "source": "producthunt",
        }


class AIToolsDirScraper(BaseScraper):
    """AI Tools Directory æŠ“å–å™¨ï¼ˆèšåˆå¤šä¸ªæºï¼‰"""
    
    def __init__(self):
        super().__init__("AI Tools Directory")
        self.sources = [
            "https://theresanaiforthat.com",
            "https://www.futurepedia.io",
            "https://www.aitools.fyi",
        ]
    
    async def fetch(self, limit: int = 50) -> List[Dict[str, Any]]:
        """ä»å¤šä¸ª AI ç›®å½•æŠ“å–"""
        all_tools = []
        
        for source_url in self.sources:
            try:
                tools = await self._fetch_from_source(source_url)
                all_tools.extend(tools)
                print(f"âœ“ ä» {source_url} æŠ“å–åˆ° {len(tools)} ä¸ªå·¥å…·")
            except Exception as e:
                print(f"âœ— ä» {source_url} æŠ“å–å¤±è´¥: {e}")
        
        return all_tools[:limit]
    
    async def _fetch_from_source(self, url: str) -> List[Dict[str, Any]]:
        """ä»å•ä¸ªæºæŠ“å–"""
        response = await self.client.get(url, headers={
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
        })
        response.raise_for_status()
        
        # ç®€åŒ–å¤„ç†ï¼šè¿”å›ç©ºåˆ—è¡¨ï¼ˆå®é™…é¡¹ç›®ä¸­éœ€è¦é’ˆå¯¹æ¯ä¸ªç½‘ç«™å®šåˆ¶è§£æé€»è¾‘ï¼‰
        return []


async def main():
    """ä¸»å‡½æ•° - æµ‹è¯•æŠ“å–å™¨"""
    scrapers = [
        GitHubTrendingScraper(),
        ProductHuntScraper(),
        #AIToolsDirScraper(),
    ]
    
    results = {}
    
    for scraper in scrapers:
        print(f"\nğŸ” æ­£åœ¨æŠ“å–: {scraper.name}")
        tools = await scraper.fetch()
        results[scraper.name] = tools
        print(f"âœ“ è·å–åˆ° {len(tools)} ä¸ªå·¥å…·")
    
    # ä¿å­˜ç»“æœ
    output_dir = os.path.join(os.path.dirname(__file__), '..', 'data')
    os.makedirs(output_dir, exist_ok=True)
    
    output_file = os.path.join(output_dir, 'scraped_tools.json')
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ’¾ æ•°æ®å·²ä¿å­˜åˆ°: {output_file}")
    
    # å…³é—­æ‰€æœ‰å®¢æˆ·ç«¯
    for scraper in scrapers:
        await scraper.close()


if __name__ == "__main__":
    import asyncio
    asyncio.run(main())
