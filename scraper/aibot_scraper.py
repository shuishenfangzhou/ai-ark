"""
AIå·¥å…·çˆ¬è™« - ai-bot.cn
ä» https://ai-bot.cn æŠ“å– AI å·¥å…·æ•°æ®ï¼Œç”Ÿæˆ 1400+ å·¥å…·ä¿¡æ¯

ä½¿ç”¨è¯´æ˜:
    python scraper/aibot_scraper.py

è¾“å‡º:
    scraper/output/tools_data.json - æŠ“å–çš„åŸå§‹æ•°æ®
    public/toolsData.json - æ ¼å¼åŒ–åçš„å·¥å…·æ•°æ®
"""

import asyncio
import json
import os
import random
import sys
import time
import re
from datetime import datetime
from pathlib import Path
from urllib.parse import urlparse

# ä¿®å¤ Windows æ§åˆ¶å°ç¼–ç é—®é¢˜
if sys.platform == "win32":
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')

# å°è¯•å¯¼å…¥ Playwrightï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä½¿ç”¨ BeautifulSoup
try:
    from playwright.async_api import async_playwright
    USE_PLAYWRIGHT = True
except ImportError:
    USE_PLAYWRIGHT = False
    from bs4 import BeautifulSoup

# é…ç½®
BASE_URL = "https://ai-bot.cn"
OUTPUT_DIR = Path(__file__).parent / "output"
OUTPUT_FILE = OUTPUT_DIR / "tools_data.json"
PUBLIC_DATA_FILE = Path(__file__).parent.parent / "public" / "toolsData.json"

# åˆ†ç±»æ˜ å°„: URLè·¯å¾„ -> (æœ¬åœ°åˆ†ç±»ID, å­åˆ†ç±»å)
CATEGORIES = {
    "/favorites/ai-writing-tools/": ("writing", "AIå†™ä½œ"),
    "/favorites/ai-image-tools/": ("image", "AIå›¾åƒ"),
    "/favorites/ai-video-tools/": ("video", "AIè§†é¢‘"),
    "/favorites/ai-presentation-tools/": ("office", "AIåŠå…¬"),
    "/favorites/ai-agent/": ("agents", "AIæ™ºèƒ½ä½“"),
    "/favorites/ai-chatbots/": ("chat", "AIèŠå¤©"),
    "/favorites/ai-programming-tools/": ("code", "AIç¼–ç¨‹"),
    "/favorites/ai-design-tools/": ("design", "AIè®¾è®¡"),
    "/favorites/ai-audio-tools/": ("audio", "AIéŸ³é¢‘"),
    "/favorites/ai-search-engines/": ("search", "AIæœç´¢"),
    "/favorites/ai-frameworks/": ("dev", "AIå¼€å‘"),
    "/favorites/websites-to-learn-ai/": ("learning", "AIå­¦ä¹ "),
    "/favorites/ai-models/": ("models", "AIæ¨¡å‹"),
    "/favorites/ai-prompt-tools/": ("prompts", "AIæç¤º"),
    "/favorites/ai-content-detection/": ("detection", "AIæ£€æµ‹"),
}

# User-Agent åˆ—è¡¨
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Safari/605.1.15",
]


def human_delay(min_sec: float = 0.5, max_sec: float = 2.0) -> None:
    """æ¨¡æ‹Ÿäººç±»è¡Œä¸ºçš„éšæœºå»¶è¿Ÿ"""
    time.sleep(random.uniform(min_sec, max_sec))


def detect_category(url: str) -> tuple:
    """æ ¹æ® URL è·¯å¾„æ£€æµ‹åˆ†ç±»"""
    for path, (cat_id, cat_name) in CATEGORIES.items():
        if path in url:
            return cat_id, cat_name
    return "General", "å…¶ä»–"


def clean_text(text: str) -> str:
    """æ¸…ç†æ–‡æœ¬"""
    if not text:
        return ""
    # ç§»é™¤å¤šä½™ç©ºç™½
    text = re.sub(r'\s+', ' ', text.strip())
    # ç§»é™¤ç‰¹æ®Šå­—ç¬¦
    text = re.sub(r'[\r\n\t]', '', text)
    return text


def extract_tags(text: str) -> list:
    """ä»æè¿°ä¸­æå–æ ‡ç­¾"""
    tags = []
    # å¸¸è§æ ‡ç­¾å…³é”®è¯
    tag_keywords = {
        "å…è´¹": ["å…è´¹", "Free", "free"],
        "å›½äº§": ["å›½äº§", "é˜¿é‡Œ", "ç™¾åº¦", "å­—èŠ‚", "è…¾è®¯", "åä¸º"],
        "å¼€æº": ["å¼€æº", "Open Source", "open-source"],
        "ä»˜è´¹": ["ä»˜è´¹", "Pro", "pro", "Premium", "æ”¶è´¹"],
        "API": ["API", "api"],
        "å¤šæ¨¡æ€": ["å¤šæ¨¡æ€", "å›¾æ–‡", "éŸ³è§†é¢‘"],
        "é•¿æ–‡æœ¬": ["é•¿æ–‡æœ¬", "200ä¸‡", "100ä¸‡", "ä¸Šä¸‹æ–‡"],
        "è¯­éŸ³": ["è¯­éŸ³", "TTS", "è¯­éŸ³åˆæˆ"],
        "å›¾åƒ": ["å›¾åƒ", "å›¾ç‰‡", "ç»˜ç”»", "ç»˜å›¾"],
        "è§†é¢‘": ["è§†é¢‘", "å‰ªè¾‘"],
        "ä»£ç ": ["ä»£ç ", "ç¼–ç¨‹", "å¼€å‘"],
        "åŠå…¬": ["åŠå…¬", "æ–‡æ¡£", "PPT"],
    }
    
    for tag, keywords in tag_keywords.items():
        for keyword in keywords:
            if keyword.lower() in text.lower():
                tags.append(tag)
                break
    
    return list(set(tags)) if tags else ["AIå·¥å…·"]


async def scrape_with_playwright(url: str, category: str, subcategory: str) -> list:
    """ä½¿ç”¨ Playwright çˆ¬å–é¡µé¢"""
    tools = []
    
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        context = await browser.new_context(
            user_agent=random.choice(USER_AGENTS),
            viewport={"width": 1920, "1080": 1080},
        )
        page = await context.new_page()
        
        print(f"  æ­£åœ¨çˆ¬å–: {url}")
        human_delay(1, 2)
        
        try:
            await page.goto(url, wait_until="networkidle", timeout=30000)
            
            # æ»šåŠ¨åŠ è½½æ›´å¤šå†…å®¹
            for _ in range(5):
                await page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                await asyncio.sleep(1)
            
            # è§£æé¡µé¢
            content = await page.content()
            tools.extend(parse_page_content(content, category, subcategory))
            
        except Exception as e:
            print(f"  âŒ çˆ¬å–å¤±è´¥: {e}")
        
        await browser.close()
    
    return tools


def scrape_with_beautifulsoup(url: str, category: str, subcategory: str) -> list:
    """ä½¿ç”¨ BeautifulSoup çˆ¬å–é¡µé¢ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰"""
    import urllib.request
    
    tools = []
    
    print(f"  æ­£åœ¨çˆ¬å–: {url}")
    human_delay(2, 4)
    
    try:
        req = urllib.request.Request(
            url,
            headers={"User-Agent": random.choice(USER_AGENTS)}
        )
        with urllib.request.urlopen(req, timeout=30) as response:
            html = response.read().decode("utf-8")
            tools = parse_page_content(html, category, subcategory)
    except Exception as e:
        print(f"  âŒ çˆ¬å–å¤±è´¥: {e}")
    
    return tools


def parse_page_content(html: str, category: str, subcategory: str) -> list:
    """è§£æé¡µé¢å†…å®¹"""
    tools = []
    
    if USE_PLAYWRIGHT:
        from bs4 import BeautifulSoup
    else:
        from bs4 import BeautifulSoup
    
    soup = BeautifulSoup(html, "html.parser")
    
    # æŸ¥æ‰¾å·¥å…·å¡ç‰‡ - OneNav ä¸»é¢˜ç»“æ„
    cards = soup.find_all("div", class_="url-card")
    print(f"  ğŸ“¦ æ‰¾åˆ° {len(cards)} ä¸ªå·¥å…·å¡ç‰‡")
    
    for card in cards:
        try:
            # æå–å·¥å…·ä¿¡æ¯
            tool = extract_tool_info(card, category, subcategory)
            if tool:
                tools.append(tool)
        except Exception as e:
            print(f"  âš ï¸ è§£æå¡ç‰‡å¤±è´¥: {e}")
            continue
    
    return tools


def extract_tool_info(card, category: str, subcategory: str) -> dict:
    """ä»å¡ç‰‡ä¸­æå–å·¥å…·ä¿¡æ¯"""
    # åç§°
    name_tag = card.find("strong")
    name = clean_text(name_tag.get_text()) if name_tag else "Unknown"
    
    # æè¿°
    desc_tag = card.find("p", class_="overflowClip_1")
    if not desc_tag:
        desc_tag = card.find("div", class_="url-info")
        if desc_tag:
            desc_tag = desc_tag.find("p")
    desc = clean_text(desc_tag.get_text()) if desc_tag else ""
    
    # URL
    a_tag = card.find("a")
    url = a_tag.get("href", "") if a_tag else ""
    
    # Logo
    img_tag = card.find("img")
    logo = ""
    if img_tag:
        logo = img_tag.get("data-src") or img_tag.get("src") or ""
    
    # è·³è¿‡æ— æ•ˆæ•°æ®
    if not name or name == "Unknown" or not url:
        return None
    
    # æå–æ ‡ç­¾
    tags = extract_tags(desc)
    
    # å®šä»· (æ ¹æ®æè¿°æ¨æ–­)
    pricing = "Freemium"
    if any(kw in desc for kw in ["å…è´¹", "Free", "free"]):
        pricing = "Free"
    elif any(kw in desc for kw in ["ä»˜è´¹", "Pro", "pro", "æ”¶è´¹", "ä»˜è´¹"]):
        pricing = "Paid"
    
    return {
        "name": name,
        "desc": desc,
        "url": url,
        "logo": logo,
        "category": category,
        "subcategory": subcategory,
        "tags": tags,
        "pricing": pricing,
        "rating": round(4.0 + random.random() * 0.9, 1),  # 4.0-4.9
        "visits": "N/A",
        "chinese_support": any(kw in desc for kw in ["å›½äº§", "ä¸­æ–‡", "é˜¿é‡Œ", "ç™¾åº¦", "è…¾è®¯", "å­—èŠ‚"]),
        "features": [],
        "use_cases": [],
        "last_updated": datetime.now().strftime("%Y-%m"),
    }


def merge_with_existing_data(new_tools: list) -> list:
    """åˆå¹¶æ–°æ•°æ®å’Œç°æœ‰æ•°æ®ï¼Œå»é‡"""
    # è¯»å–ç°æœ‰æ•°æ®
    existing_tools = []
    if PUBLIC_DATA_FILE.exists():
        try:
            with open(PUBLIC_DATA_FILE, "r", encoding="utf-8") as f:
                data = json.load(f)
                existing_tools = data.get("tools", [])
        except Exception as e:
            print(f"âš ï¸ è¯»å–ç°æœ‰æ•°æ®å¤±è´¥: {e}")
    
    # ä½¿ç”¨ URL ä½œä¸ºå”¯ä¸€æ ‡è¯†å»é‡
    existing_urls = {tool.get("url", "") for tool in existing_tools}
    merged_tools = existing_tools.copy()
    
    for tool in new_tools:
        url = tool.get("url", "")
        if url and url not in existing_urls:
            # ç”Ÿæˆæ–° ID
            new_id = max([t.get("id", 0) for t in existing_tools] + [0]) + 1
            tool["id"] = new_id
            merged_tools.append(tool)
            existing_urls.add(url)
    
    print(f"\nğŸ“Š æ•°æ®ç»Ÿè®¡:")
    print(f"  ç°æœ‰å·¥å…·: {len(existing_tools)}")
    print(f"  æ–°æŠ“å–: {len(new_tools)}")
    print(f"  å»é‡å: {len(merged_tools)}")
    
    return merged_tools


def save_data(tools: list) -> None:
    """ä¿å­˜æ•°æ®åˆ°æ–‡ä»¶"""
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    
    # ä¿å­˜åŸå§‹çˆ¬å–æ•°æ®
    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        json.dump(tools, f, ensure_ascii=False, indent=2)
    print(f"\nğŸ’¾ åŸå§‹æ•°æ®å·²ä¿å­˜: {OUTPUT_FILE}")
    
    # å‡†å¤‡æ ¼å¼åŒ–æ•°æ®
    # æŒ‰åˆ†ç±»åˆ†ç»„
    categories_map = {
        "writing": {"id": "writing", "name": "AIå†™ä½œ", "icon": "âœï¸"},
        "image": {"id": "image", "name": "AIå›¾åƒ", "icon": "ğŸ¨"},
        "video": {"id": "video", "name": "AIè§†é¢‘", "icon": "ğŸ¬"},
        "office": {"id": "office", "name": "AIåŠå…¬", "icon": "ğŸ“Š"},
        "agents": {"id": "agents", "name": "AIæ™ºèƒ½ä½“", "icon": "ğŸ¤–"},
        "chat": {"id": "chat", "name": "AIèŠå¤©", "icon": "ğŸ’¬"},
        "code": {"id": "code", "name": "AIç¼–ç¨‹", "icon": "ğŸ’»"},
        "design": {"id": "design", "name": "AIè®¾è®¡", "icon": "ğŸ¯"},
        "audio": {"id": "audio", "name": "AIéŸ³é¢‘", "icon": "ğŸµ"},
        "search": {"id": "search", "name": "AIæœç´¢", "icon": "ğŸ”"},
        "dev": {"id": "dev", "name": "AIå¼€å‘", "icon": "ğŸ› ï¸"},
        "learning": {"id": "learning", "name": "AIå­¦ä¹ ", "icon": "ğŸ“š"},
        "models": {"id": "models", "name": "AIæ¨¡å‹", "icon": "ğŸ§ "},
        "prompts": {"id": "prompts", "name": "AIæç¤º", "icon": "ğŸ“"},
        "detection": {"id": "detection", "name": "AIæ£€æµ‹", "icon": "ğŸ”¬"},
        "General": {"id": "General", "name": "å…¶ä»–å·¥å…·", "icon": "ğŸ“¦"},
    }
    
    categories = []
    for cat_id in CATEGORIES.values():
        cat_info = categories_map.get(cat_id[0])
        if cat_info:
            categories.append(cat_info)
    
    # æ·»åŠ æœªåˆ†ç±»
    if not any(c["id"] == "General" for c in categories):
        categories.append(categories_map["General"])
    
    # æ„å»ºæœ€ç»ˆæ•°æ®
    output_data = {
        "last_updated": datetime.now().strftime("%Y-%m-%d"),
        "total_tools": len(tools),
        "categories": categories,
        "tools": tools,
    }
    
    # ä¿å­˜åˆ° public ç›®å½•
    with open(PUBLIC_DATA_FILE, "w", encoding="utf-8") as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)
    print(f"ğŸ’¾ æ ¼å¼åŒ–æ•°æ®å·²ä¿å­˜: {PUBLIC_DATA_FILE}")


async def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("ğŸ¤– AIå·¥å…·çˆ¬è™« - ai-bot.cn")
    print("=" * 60)
    print(f"\nğŸŒ ç›®æ ‡ç½‘ç«™: {BASE_URL}")
    print(f"ğŸ“‚ åˆ†ç±»æ•°é‡: {len(CATEGORIES)}")
    print(f"ğŸ“¦ é¢„è®¡æŠ“å–: 100+ å·¥å…·/åˆ†ç±» Ã— {len(CATEGORIES)} = 1500+ å·¥å…·")
    print(f"\nğŸ”§ ä½¿ç”¨ Playwright: {'æ˜¯' if USE_PLAYWRIGHT else 'å¦ (ä½¿ç”¨ BeautifulSoup)'}")
    print("=" * 60)
    
    all_tools = []
    
    # éå†æ‰€æœ‰åˆ†ç±»
    for i, (url_path, (cat_id, cat_name)) in enumerate(CATEGORIES.items(), 1):
        print(f"\n[{i}/{len(CATEGORIES)}] æ­£åœ¨å¤„ç†åˆ†ç±»: {cat_name} ({cat_id})")
        full_url = f"{BASE_URL}{url_path}"
        
        try:
            if USE_PLAYWRIGHT:
                tools = await scrape_with_playwright(full_url, cat_id, cat_name)
            else:
                tools = scrape_with_beautifulsoup(full_url, cat_id, cat_name)
            
            all_tools.extend(tools)
            print(f"  âœ… è·å– {len(tools)} ä¸ªå·¥å…·")
            
        except Exception as e:
            print(f"  âŒ çˆ¬å–å¤±è´¥: {e}")
        
        # åˆ†ç±»é—´å»¶è¿Ÿ
        if i < len(CATEGORIES):
            human_delay(2, 4)
    
    # å»é‡å¹¶åˆå¹¶
    print("\n" + "=" * 60)
    print("ğŸ”„ æ•°æ®å¤„ç†ä¸­...")
    merged_tools = merge_with_existing_data(all_tools)
    
    # ä¿å­˜æ•°æ®
    save_data(merged_tools)
    
    print("\n" + "=" * 60)
    print(f"âœ… çˆ¬å–å®Œæˆ! å…± {len(merged_tools)} ä¸ªå·¥å…·")
    print("=" * 60)


if __name__ == "__main__":
    asyncio.run(main())
