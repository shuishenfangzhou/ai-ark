"""
GitHub Trending æŠ“å–è„šæœ¬ - å¤‡ç”¨ç‰ˆæœ¬ï¼ˆæ—  aiohttpï¼‰

ä½¿ç”¨ requests åº“ï¼Œé€‚ç”¨äºæ²¡æœ‰å®‰è£… aiohttp çš„ç¯å¢ƒ

ä½¿ç”¨è¯´æ˜:
    python scraper/github_trending_simple.py

è¾“å‡º:
    scraper/output/github_data.json - GitHub å·¥å…·æ•°æ®
"""

import json
import os
import sys
import time
from datetime import datetime
from pathlib import Path
from urllib.request import Request, urlopen
from urllib.parse import urlencode
import ssl
import re

# æ·»åŠ å½“å‰ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))
from config.github_topics import (
    GITHUB_TOPICS,
    EXCLUDED_TOPICS,
    PER_PAGE,
    REQUEST_DELAY,
    MIN_STARS
)

# é…ç½®
OUTPUT_DIR = Path(__file__).parent / "output"
OUTPUT_FILE = OUTPUT_DIR / "github_data.json"

# GitHub API
GITHUB_API_BASE = "https://api.github.com"

# è¯·æ±‚å¤´
HEADERS = {
    "Accept": "application/vnd.github.v3+json",
    "User-Agent": "AI-Tools-Scraper/1.0"
}

# GitHub Token
GITHUB_TOKEN = os.getenv("GITHUB_TOKEN", "")
if GITHUB_TOKEN:
    HEADERS["Authorization"] = f"token {GITHUB_TOKEN}"


def make_request(url, retries=3):
    """å‘é€ HTTP è¯·æ±‚"""
    for attempt in range(retries):
        try:
            req = Request(url, headers=HEADERS)
            
            # SSL ä¸Šä¸‹æ–‡
            ctx = ssl.create_default_context()
            ctx.check_hostname = False
            ctx.verify_mode = ssl.CERT_NONE
            
            with urlopen(req, timeout=30, context=ctx) as response:
                data = response.read().decode('utf-8')
                return json.loads(data)
                
        except Exception as e:
            print(f"  è¯·æ±‚å¤±è´¥ ({attempt + 1}/{retries}): {e}")
            time.sleep(REQUEST_DELAY * 2)
    
    return None


def search_repositories(query):
    """æœç´¢ä»“åº“"""
    url = f"{GITHUB_API_BASE}/search/repositories?{urlencode({
        'q': f'{query} stars:>={MIN_STARS}',
        'sort': 'stars',
        'order': 'desc',
        'per_page': PER_PAGE
    })}"
    
    data = make_request(url)
    if data and "items" in data:
        return data["items"]
    return []


def get_repository_details(full_name):
    """è·å–ä»“åº“è¯¦ç»†ä¿¡æ¯"""
    url = f"{GITHUB_API_BASE}/repos/{full_name}"
    return make_request(url)


def categorize_tool(topics, language):
    """åˆ†ç±»å·¥å…·"""
    topic_str = " ".join(topics).lower()
    
    if any(t in topic_str for t in ["llm", "large-language-model", "gpt", "transformer"]):
        return "dev"
    if any(t in topic_str for t in ["machine-learning", "deep-learning"]):
        return "dev"
    if any(t in topic_str for t in ["stable-diffusion", "image-generation"]):
        return "image"
    if any(t in topic_str for t in ["nlp", "natural-language-processing"]):
        return "writing"
    if any(t in topic_str for t in ["video-generation"]):
        return "video"
    if any(t in topic_str for t in ["speech-recognition", "text-to-speech"]):
        return "audio"
    if any(t in topic_str for t in ["agent", "langchain"]):
        return "agents"
    
    return "dev"


def parse_description(description):
    """æ¸…ç†æè¿°"""
    if not description:
        return ""
    description = re.sub(r'[^\w\s\-\.\,\(\)]', '', description)
    return description.strip()[:500]


def scrape_category(category, topics):
    """æŠ“å–å•ä¸ªåˆ†ç±»"""
    print(f"\nğŸ” æŠ“å–åˆ†ç±»: {category}")
    category_tools = []
    
    for topic in topics[:5]:
        print(f"  ğŸ“Œ ä¸»é¢˜: {topic}")
        
        repos = search_repositories(topic)
        print(f"    æ‰¾åˆ° {len(repos)} ä¸ªä»“åº“")
        
        for repo in repos[:10]:
            full_name = repo.get("full_name", "")
            print(f"    å¤„ç†: {full_name}")
            
            # è·å–è¯¦ç»†ä¿¡æ¯
            details = get_repository_details(full_name)
            if not details:
                time.sleep(REQUEST_DELAY)
                continue
            
            topics_list = details.get("topics", [])
            
            # è·³è¿‡æ’é™¤çš„
            for excluded in EXCLUDED_TOPICS:
                if excluded in topics_list:
                    continue
            
            language = details.get("language", "")
            tool_category = categorize_tool(topics_list, language)
            
            owner = details.get("owner", {})
            
            tool = {
                "id": details.get("id", 0),
                "name": details.get("name", ""),
                "category": tool_category,
                "subcategory": category,
                "desc": parse_description(details.get("description", "")),
                "url": details.get("html_url", ""),
                "github_url": details.get("html_url", ""),
                "tags": topics_list[:10],
                "pricing": "Free",
                "rating": min(5.0, 3.0 + (details.get("stargazers_count", 0) / 100000)),
                "visits": f"{details.get('stargazers_count', 0)}",
                "logo": owner.get("avatar_url", "") if owner else "",
                "stars": details.get("stargazers_count", 0),
                "forks": details.get("forks_count", 0),
                "language": language,
                "updated_at": details.get("updated_at", "")[:10]
            }
            
            if tool["id"] not in [t.get("id") for t in category_tools]:
                category_tools.append(tool)
                print(f"      âœ… {tool['name']} ({tool['stars']} â­)")
            
            time.sleep(REQUEST_DELAY)
        
        time.sleep(REQUEST_DELAY * 2)
    
    return category_tools


def run():
    """è¿è¡ŒæŠ“å–"""
    print("=" * 60)
    print("ğŸš€ GitHub AI å·¥å…·æŠ“å–è„šæœ¬ï¼ˆç®€å•ç‰ˆï¼‰")
    print("=" * 60)
    print(f"ğŸ“… æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    all_tools = []
    
    # æŠ“å–å„åˆ†ç±»
    for category, topics in GITHUB_TOPICS.items():
        tools = scrape_category(category, topics)
        all_tools.extend(tools)
        print(f"  ğŸ“Š {category}: {len(tools)} ä¸ªå·¥å…·")
    
    # å»é‡
    seen_ids = set()
    unique_tools = []
    for tool in all_tools:
        if tool["id"] not in seen_ids:
            seen_ids.add(tool["id"])
            unique_tools.append(tool)
    
    # æ’åº
    unique_tools.sort(key=lambda x: x.get("stars", 0), reverse=True)
    
    # ä¿å­˜
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    
    final_data = {
        "last_updated": datetime.now().strftime("%Y-%m-%d"),
        "total_tools": len(unique_tools),
        "sources": ["github_topics"],
        "tools": unique_tools
    }
    
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        json.dump(final_data, f, ensure_ascii=False, indent=2)
    
    print()
    print("=" * 60)
    print("âœ… æŠ“å–å®Œæˆ!")
    print(f"ğŸ“Š æ€»è®¡: {len(unique_tools)} ä¸ªå·¥å…·")
    print(f"ğŸ’¾ ä¿å­˜åˆ°: {OUTPUT_FILE}")
    print("=" * 60)


if __name__ == "__main__":
    run()
